import os
import sys
import streamlit as st
from utils import *
from utils import load_llm

openai_key = st.secrets["OPENAI_API_KEY"]
os.environ["OPENAI_API_KEY"] = openai_key


async def main():
    #### WEB APP Code ####
    st.title("Hello - 5C Hachathon")
    st.write(
        "This is a simple web app to demonstrate the deployment of a machine learning model using Streamlit."
    )

    # user input
    st.sidebar.header("User Input Parameters")
    st.sidebar.write("What do you want to study/research?")
    user_input = st.sidebar.text_input("Describe your interest ", "Type here...")

    # Call Google Scholar API to get results, then parse with chat
    params = {
        "engine": "google_scholar",
        "q": user_input,
        "api_key": os.getenv("SERP_API_KEY"),
        "as_ylo": 2018,
        "as_yhi": 2024,
        "hl": "en",
        "start": 0,
        "num": 3,  # limited to 20
        "output": "json",
    }

    # Get results from Google Scholar
    results, organic_results = search_scholar(params)

    # Parse the JSON data and return a DataFrame
    ## Citations can be a proxy for author/publication influence level
    df = parse_json_to_df(organic_results)

    # Collect structured responses
    ## Not entirely sure that this is actually going asynchronously because I see progress bars one by one
    structured_responses = await collect_webscraping(df)

    # Get the information from the structured responses
    res, link = get_info(
        structured_responses, 0
    )  # Will later need to make this for all responses, right now it's just one

    print(res, link)

    # Output in Streamlit
    st.write(f"Response: {res}")
    st.write(f"Link: {link}")

    """Now, we have a list of authors/institutions"""

    sys.exit(0)

    llm_test = load_llm(provider="openai", max_tokens=100, temperature=0.75)
    llm_response = llm_test.invoke(
        f"Create a google scholar search query to fetch papers relevant to the user interests: {user_input}"
    )

    st.write(f"User input: {llm_response.content}")

    ## Parse the text generated by LLM into a list or dictionary of author-institution pairs
    parsed_data = [
        "Author 1 - Institution 1",
        "Author 2 - Institution 2",
        "Author 3 - Institution 3",
    ]

    st.markdown("---")

    # dropdown is populated with the parsed data
    st.markdown("## Select the author-institution pair")
    user_choice = st.selectbox("Author-Institution Pair", parsed_data)

    st.write(f"User choice: {user_choice}")

    # button to submit the choice
    if st.button("Submit"):
        st.write("You have submitted the choice")

        # user_choice is used here to fetch the data from the database or API
        ## We need information about the author from some public API
        ## Institution information can be fetched from the database

        st.write("Thank you for using the web app!")


if __name__ == "__main__":
    asyncio.run(main())
